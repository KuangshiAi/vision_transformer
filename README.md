# When Fine-tuning always Beats Training from Scratch: Strategies to Adopt in Vison Transformer Training
This repository gives implementation of our proposed method, and also experimental code of other models for comparison in our paper.

Training from scratch test code and result on tf_flower, CIFAR10, CIFAR100 and Tiny-ImageNet are listed in folders named by model, including original ViT-Ti, SL-ViT, DeiT and our proposed model. Transfering pre-trained ViT models are mainly work done by Google Research, we made some modification for training on our chosen datasets.

The implementations are basically made by tensorflow and torch, all jupyter notebook can be run directly on Colab (with support of Colab Pro).
## Disclaimers
Author: Kuangshi Ai, Jiaying He
Email: kuai6409@uni.sydney.edu.au, jihe7032@uni.sydney.edu.au
Assignment 2 Research Track for COMP5329 (USYD 2023), Sydney, Australia

Note: Some part of this repository was forked and modified from open-sourced
[google-research/vision_transformer](https://github.com/google-research/vision_transformer).

